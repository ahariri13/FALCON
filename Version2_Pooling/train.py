# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1arKvUWK7g_3ARG6tpw2PxXvP-V90MpeS

In this code, we aim to use Edge Pooling and Unpooling in order to reconstruct the events. The main message passing technique so far is SAGE, and the main loss functions are MSE and Chamfer Distance

# Libraries Installation

Import Pytorch Version 1.4 for compatibility with some libraries to be used like neural-net pytorch that contains EMD and Chamfer Distance Libraries
"""

#pip install -q torch==1.4.0 torchvision==0.i5

#pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html
#pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html
#pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html
#pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html
#pip install torch-geometric

#pip install "neuralnet-pytorch[geom] @ git+git://github.com/justanhduc/neuralnet-pytorch.git@master"

#pip install neuralnet-pytorch

#pip install git+git://github.com/justanhduc/neuralnet-pytorch.git@fancy

#from google.colab import drive
#drive.mount('/content/drive')

#cd drive/My\ Drive/gae/fast_gae/ECAL/temp

import argparse
import time
import torch_geometric
import numpy as np
import scipy.sparse as sp
import torch
from torch import optim
from torch_geometric.data import Data

from torch.optim.lr_scheduler import StepLR
import torch.nn.functional as F
from torch.utils.data import Dataset, IterableDataset, DataLoader
from itertools import cycle, islice
from torch_geometric.utils import dropout_adj

from optimizer import loss_function
import iterableJet
from iterableJet import IterableMuons
import class_jet
from class_jet import FCMuonsGPU
from model import GraphPooling

use_cuda = torch.cuda.is_available()
device = torch.device("cuda:0" if use_cuda else "cpu")


parser = argparse.ArgumentParser()
parser.add_argument('--seed', type=int, default=42, help='Random seed.')
parser.add_argument('--epochs', type=int, default=50, help='Number of epochs to train.')
parser.add_argument('--batch_size', type=int, default=256, help='Initial learning rate.') #100

parser.add_argument('--lr', type=float, default=0.0001, help='Initial learning rate.') #0.001
parser.add_argument('--dropout', type=float, default=0.4, help='Dropout rate (1 - keep probability).')
args = parser.parse_args([])
torch.backends.cudnn.benchmark = True

samples=torch.load('./ECAL_f01_june.pt')#
#samples=torch.load('../ECAL Data /ECAL_f01_june_60k.pt')
#samples=torch.load('../padded_June/newPad_0.pt')

#samples=samples[:30000]

for sam in samples:
  sam['x'][:,2]=-torch.log(sam['x'][:,2])*15
  #sam['x'][:,:2]/=150

model = GraphPooling(in_channels=3, out_channels1=64, out_channels2=128, out_channels3=256,out_channels4=512,out_channels5=1024,dropout=args.dropout,
                    batch_size=args.batch_size)
optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=0.001)
#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.001)

def sizer(samples):
    inds=[]
    for k in samples:
        inds.append(k['x'].shape[0])
    return inds

sizes=sizer(samples[:58000])

fin=[]
countit=0
for m in sizes:
    fin.append(np.repeat(countit,m))
    countit+=1

from torch_geometric.data import DataLoader
#jet=IterableMuons(samples)
loader = DataLoader(samples[:58000], batch_size=args.batch_size,num_workers=20,pin_memory=True)

#%%
#model.train()
#model= model.to(device)
#optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.001)
#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.0005, weight_decay=0.001)
model.train()
model.to(device, non_blocking=True)

scheduler=StepLR(optimizer, step_size=4, gamma=0.1)

testing = samples[-1000:]

sizes2=sizer(testing)

fin2=[]
count23=0
for m2 in sizes2:
    fin2.append(np.repeat(count23,m2))
    count23+=1

from torch_geometric.data import DataLoader
Testloader = DataLoader(testing, batch_size=100)

#epoch2=checkpoint['epoch']

import torch.cuda.nvtx as nvtx 
for epoch in range(50):
    #model.train()
    
    count=0
    c1,c2=0,args.batch_size
    epLoss=0
    t = time.time()
    for el in islice(loader,0,290):#count2,el in enumerate(loader):
        nvtx.range_push("Batch "+str(count))
        gra=el.x
        adj=el.edge_index



        check=torch.LongTensor(np.hstack(np.array(fin[c1:c2])))
        count+=1#=count2+1
        if count==2:
            print("Started Training")
        hidden_emb = None
        
        gra=gra.cuda()
        adj=adj.cuda()
        lengs=check.cuda()
        for param in model.parameters():  
            param.grad = None
        nvtx.range_push("Forward Pass")

        r1 ,mu,sig= model(gra,adj,lengs,gra.shape[0])


        loss = loss_function(r1,gra,lengs,mu,sig) #+loss2(logsoftmax(torch.transpose(r2,1,2)),temp.long())      
        nvtx.range_pop()
        loss.backward()

        optimizer.step()

        

        cur_loss = loss.item()

        
        torch.cuda.empty_cache() 
        
        epLoss+=float(cur_loss)

        #hidden_emb = mu.data.numpy() 


        c1+=args.batch_size
        c2+=args.batch_size


        
        
        if count%145==0:
            print("Epoch:", '%04d' % (epoch + 1), "train_loss=", "{:.5f}".format(epLoss/count),"time=", "{:.5f}".format(time.time() - t))
            t = time.time()

            with torch.no_grad():
              model.eval()
              VaLoss=0
              d1,d2=0,100
              for el2 in islice(Testloader,10):
                gra2=el2.x.cuda()  ##features 
                adj2=el2.edge_index.cuda() ## edge matrix 
                lengs2=torch.LongTensor(np.hstack(np.array(fin2[d1:d2]))).cuda()
                r12,mu2,sig2= model(gra2,adj2,lengs2,gra2.shape[0]) ## X , A 
                loss2 = loss_function(r12,gra2,lengs2,mu2,sig2)
                cur_loss2 = loss2.item()
                VaLoss+=float(cur_loss2)
                d1+=100
                d2+=100
              print("Val Loss:", '%04d' % (VaLoss/10))

    if epoch%2==0:
      scheduler.step()
      #cumLoss=validate(Testloader,5)
      torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'epoch':epoch,
            'loss': loss,
            'epLoss':epLoss
            }, './edge_mse_august.pth')

